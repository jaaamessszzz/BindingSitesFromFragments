#!/usr/bin/env python3


import os
import pickle
import subprocess
import tarfile
import tempfile
from pprint import pprint

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import prody
from prody.sequence.sequence import Sequence
from scipy.spatial.distance import jensenshannon
import seaborn as sns

from Bio import SeqIO
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
from Bio.Alphabet import IUPAC
from Bio.Align.Applications import ClustalOmegaCommandline
from Bio import motifs

import weblogo
from weblogo.seq import unambiguous_protein_alphabet

from ..utils import *

# --- Figures --- #

def binding_interface_figures(args):
    """
    Generate HMMER profile similarity figures

    Usage:
      bsff benchmark binding_interface_figures <statistics_dir>

    Arguments:
      <statistics_dir>          CSV file generated by binding_interface_recovery_bm()

    Options:
      --derp                    asdf
    """
    statistics_dir = args['<statistics_dir>']

    table_headers = ['target name',
                     'accession',
                     'query name',
                     'accession',
                     'E - value',
                     'score',
                     'bias',
                     'E - value',
                     'score',
                     'bias',
                     'exp',
                     'reg',
                     'clu',
                     'ov',
                     'env',
                     'dom',
                     'rep',
                     'inc',
                     'description of target']

    consolidated_df = pd.DataFrame()

    for hmm_results in os.listdir(statistics_dir):
        print(os.path.join(statistics_dir, hmm_results))
        if hmm_results.endswith('.tsv'):
            df = pd.read_csv(os.path.join(statistics_dir, hmm_results), delim_whitespace=True, comment='#', header=None, names=table_headers)
            print(df)


# Look up Noah's CoupledMoves Native Sequence Recovery
# Look up Colin's PDZ doamin paper for metrics
def jsd_figures(args):
    """
    Generate figures for profile similarity and native sequence recovery

    Usage:
      bsff benchmark jsd_figures <statistics_dir> [options]

    Arguments:
      <statistics_dir>                      Directory generated by jensen_shannon_divergence()

    Options:
      -o=<outdir>, --output=<output>        Name of directory to output figures
    """

    stats_dir = args['<statistics_dir>']
    output_dir = args['--output'] if args['--output'] else 'Figures'

    sns.set(style="whitegrid", palette="hls")
    sns.set_context("paper", font_scale=1.5)

    # Consolidate csvs into one dataframe
    all_csvs = [os.path.join(stats_dir, csv) for csv in os.listdir(stats_dir) if csv.endswith('.csv')]
    df_list = [pd.read_csv(csv) for csv in all_csvs]
    df_all = pd.concat(df_list)

    # --- Profile Similarity (Box plots) --- #

    ps_boxplot_dir = os.path.join(output_dir, 'ProfileSimilarity_Boxplots')
    os.makedirs(ps_boxplot_dir, exist_ok=True)

    def make_boxplot(df, output_name=None):
        complex_name = df['complex'].iloc[0]
        fig, ax = plt.subplots()
        boxplot = sns.boxplot(x=df['weight'], y=df['similarity'],
                              order=['Control', '0.0', '-0.5', '-1.0', '-1.5', '-2.0', '-2.5', '-3.0', '-3.5', '-4.0',
                                     '-4.5', '-5.0', '-6.0', '-8.0', '-10.0'], ax=ax)
        boxplot.set_xticklabels(boxplot.get_xticklabels(), rotation=45)
        sns.despine(left=True)

        ax.set_ylim(0, 1)
        plt.xlabel('Weights')
        plt.ylabel('Profile Similarity')
        fig.savefig(os.path.join(ps_boxplot_dir, f'ProfileSimilarity-{output_name if output_name else complex_name}.png'), bbox_inches='tight')
        plt.close()

    for df in df_list:
        make_boxplot(df)

    # Aggregate information from all complexes
    make_boxplot(df_all, output_name='Aggregate')
    df_weight_pivot = pd.pivot_table(df_all, values='similarity', index=['weight'],
                                     aggfunc={'similarity': [np.median, np.mean]})

    # Remove positions with >0.9 profile similarity in all packer conditions
    for df in df_list:
        variable_positions_df_list = []
        for (complex, position), complex_df in df.groupby(['complex', 'position']):
            if all(complex_df['similarity'] < 0.9):
                variable_positions_df_list.append(complex_df)
        variable_positions_df = pd.concat(variable_positions_df_list)
        make_boxplot(variable_positions_df, output_name=f'Variable-{complex}')

    variable_positions_df_list = []
    for (complex, position), complex_df in df_all.groupby(['complex', 'position']):
        if all(complex_df['similarity'] < 0.9):
            variable_positions_df_list.append(complex_df)
    variable_positions_df = pd.concat(variable_positions_df_list)
    make_boxplot(variable_positions_df, output_name='Variable-Aggregate')
    df_weight_pivot.to_csv(os.path.join(ps_boxplot_dir, 'ProfileSimilarity-AggregateStats.csv'))

    # --- Profile Similarity (Scatter plots) --- #

    ps_scatterplot_dir = os.path.join(output_dir, 'ProfileSimilarity_Scatterplots')
    os.makedirs(ps_scatterplot_dir, exist_ok=True)

    def make_scatterplot(df):
        fig, ax = plt.subplots(figsize=(3,3))
        sns.scatterplot('control_similarity', 'similarity', legend="full", palette=['#007CBE'], ax=ax, data=df)
        ax.plot(ax.get_xlim(), ax.get_ylim(), ls="--", c="k")
        ax.tick_params(labelsize=10)
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.set_xlabel('Profile Similarity,\nUnmodified Packer', fontsize=12)
        ax.set_ylabel(f'Profile Similarity,\nspecial_rot = {weight}', fontsize=12)
        fig.savefig(os.path.join(ps_scatterplot_dir, f'ProfileSimilarity_scatter_{weight}.png'), bbox_inches='tight', dpi=300)
        plt.close()

    annotated_df_list = []
    for (complex, position), complex_df in df_all.groupby(['complex', 'position']):
        control_ps = complex_df[complex_df['weight'] == 'Control']['similarity'].iloc[0]
        complex_df['control_similarity'] = complex_df.apply(lambda row: control_ps, axis=1)
        annotated_df_list.append(complex_df)
    annotated_df = pd.concat(annotated_df_list)

    # Profile similarity per weights vs control
    for weight, weight_df in annotated_df.groupby('weight'):
        make_scatterplot(weight_df)

    # Plot Overlay of nice example weights
    example_weights = [0.0, -1.5, -4.0]
    example_df = annotated_df[(annotated_df['weight'].isin([str(a) for a in example_weights]))]
    example_df['weight'] = example_df['weight'].astype(float)

    fig, axes = plt.subplots(1, 3, figsize=(9, 3), constrained_layout=True)

    for weight, weight_df in example_df.groupby('weight'):
        index = example_weights.index(weight)
        sns.scatterplot('control_similarity', 'similarity', legend=None, palette=['#007CBE'], ax=axes[index], data=weight_df)
        axes[index].plot(axes[index].get_xlim(), axes[index].get_ylim(), ls="--", c="k")
        axes[index].tick_params(labelsize=10)
        axes[index].set_xlim(0, 1)
        axes[index].set_ylim(0, 1)
        axes[index].set_xlabel('Profile Similarity,\nUnmodified Packer', fontsize=12)
        axes[index].set_ylabel(f'Profile Similarity,\nspecial_rot = {weight}', fontsize=12)

    fig.savefig(os.path.join(ps_scatterplot_dir, f'ProfileSimilarity_scatter_MainText.png'), bbox_inches='tight', dpi=300)
    plt.close()
        
    # --- Sequence Recovery (Bar Chart) --- #

    sr_barchart_dir = os.path.join(output_dir, 'SequenceRecovery_Barcharts')
    os.makedirs(sr_barchart_dir, exist_ok=True)

    def make_barchart(df, output_name=None):
        complex_name = df['complex'][0]

        list_of_dicts = list()

        for weight, df_sub in df.groupby('weight'):
            list_of_dicts.append({'weight': weight, 'count': df_sub['recovered'].value_counts()[True] / len(df_sub['recovered'])})

        count_df = pd.DataFrame(list_of_dicts)

        plt.figure()
        barplot = sns.barplot(x=count_df['weight'], y=count_df['count'],
                              order=['Control', '0.0', '-0.5', '-1.0', '-1.5', '-2.0', '-2.5', '-3.0', '-3.5', '-4.0',
                                     '-4.5', '-5.0', '-6.0', '-8.0', '-10.0'],
                              linewidth=1.5, edgecolor=".15",)
        barplot.set_xticklabels(barplot.get_xticklabels(), rotation=45)
        sns.despine(left=True)

        fig = barplot.get_figure()
        plt.ylim(0, 1)
        plt.xlabel('Weights')
        plt.ylabel('Fraction Recovered')
        fig.savefig(os.path.join(sr_barchart_dir, f'SeqeuenceRecovery-{output_name if output_name else complex_name}.png'), bbox_inches='tight')
        plt.close()

    for df in df_list:
        make_barchart(df)

    # Aggregate information from all complexes
    make_barchart(df_all, output_name='Aggregate')

    entropy_plots(df_all, output_dir=output_dir)


def entropy_binned_profile_similarity(df_all):
    """
    Literally replicating the figure in Noah's CoupledMoves paper...
    This definitely doesn't work in this context.
    """

    variable_positions_df_list = []
    for (complex, position), complex_df in df_all.groupby(['complex', 'position']):
        if all(complex_df['similarity'] < 0.9):
            variable_positions_df_list.append(complex_df)
    variable_positions_df = pd.concat(variable_positions_df_list)
    
    # Determine entropy bins from unmodified packer (Control) sequences
    control_df = variable_positions_df.groupby('weight').get_group('Control')
    entropies = control_df['shannon_entropy'].sort_values().tolist()
    bin_cutoff_index = int(len(entropies)/3) - 1

    first_bin_upper = entropies[bin_cutoff_index]
    second_bin_upper = entropies[-bin_cutoff_index]

    # Get dataframes based on bins
    low_entropy_df = variable_positions_df[(variable_positions_df.shannon_entropy <= first_bin_upper)]
    mid_entropy_df = variable_positions_df[(variable_positions_df.shannon_entropy > first_bin_upper) & (variable_positions_df.shannon_entropy <= second_bin_upper)]
    high_entropy_df = variable_positions_df[(variable_positions_df.shannon_entropy > second_bin_upper)]

    low_entropy_df['annotation'] = low_entropy_df.apply(lambda row: f"l_{row['weight']}", axis=1)
    mid_entropy_df['annotation'] = mid_entropy_df.apply(lambda row: f"m_{row['weight']}", axis=1)
    high_entropy_df['annotation'] = high_entropy_df.apply(lambda row: f"h_{row['weight']}", axis=1)

    new_df = pd.concat([low_entropy_df, mid_entropy_df, high_entropy_df])

    weights = sorted(list(variable_positions_df['weight'].unique()))
    plot_order = [f'{binn}_{weight}' for binn in 'lmh' for weight in weights]

    # Make figure
    plt.figure(figsize=(20,5))
    boxplot = sns.boxplot(x=new_df['annotation'], y=new_df['similarity'], order=plot_order)
    boxplot.set_xticklabels(boxplot.get_xticklabels(), rotation=45)
    sns.despine(left=True)

    fig = boxplot.get_figure()
    plt.xlabel('Weights')
    plt.ylabel('Profile Similarity')
    fig.savefig(f'EntropyBinnedProfileSimilarity.png', bbox_inches='tight', dpi=300)
    plt.close()


def entropy_plots(df_all, output_dir='Figures'):
    """
    Do RotamerSets + special_rot improve sequence diversity over unmodified Packer?
    """

    entropy_scatterplot_dir = os.path.join(output_dir, 'Entropy_Scatterplots')
    os.makedirs(entropy_scatterplot_dir, exist_ok=True)

    plot_df_list = []
    designable_positions_count = 0

    # Group dataframe by (complex, position)
    for (complex, position), df in df_all.groupby(['complex', 'position']):

        control_entropy = df.loc[df['weight'] == 'Control', 'shannon_entropy']
        for index, row in df.iterrows():
            if row['weight'] != 'Control':
                plot_df_list.append({'control': float(control_entropy),
                                     'weight': float(row['weight']),
                                     'entropy': float(row['shannon_entropy'])
                                     })
        designable_positions_count += 1

    print(f'Designable Positions: {designable_positions_count}')
    plot_df = pd.DataFrame(plot_df_list)

    def entropy_scatterplot(df, weight, palette=None):

        palette = palette if palette is not None else ['#007CBE']
        fig, ax = plt.subplots(figsize=(6, 6))
        sns.scatterplot('control', 'entropy', hue='weight', legend="full", palette=palette, ax=ax, data=df)
        ax.plot((0, 3.5), (0, 3.5), ls="--", c="k")
        ax.set_xlim(0, 3.5)
        ax.set_ylim(0, 3.5)
        ax.tick_params(labelsize=10)
        ax.set_xticks(range(0, 4, 1), minor=False)
        ax.set_yticks(range(0, 4, 1), minor=False)
        ax.set_xlabel('Shannon Entropy,\nUnmodified Packer')
        ax.set_ylabel(f'Shannon Entropy,\nspecial_rot = {weight}')
        fig.savefig(os.path.join(entropy_scatterplot_dir, f'EntropyScatter_{weight}.png'), bbox_inches='tight', dpi=300)
        plt.close()

    for weight, weight_df in plot_df.groupby('weight'):
        entropy_scatterplot(weight_df, weight)

    # MAINTEXT: Plot Overlay of nice example weights
    example_weights = [0, -1.5, -4]
    example_df = plot_df[(plot_df['weight'].isin(example_weights))]

    palette=['#007CBE', '#6EA400', '#EB093C']
    fig, axes = plt.subplots(1, 3, figsize=(9, 3), constrained_layout=True)

    for weight, weight_df in example_df.groupby('weight'):
        index = example_weights.index(weight)
        sns.scatterplot('control', 'entropy', hue='weight', legend=False, palette=['#007CBE'], ax=axes[index], data=weight_df)
        axes[index].plot((0, 3.5), (0, 3.5), ls="--", c="k")
        axes[index].set_xlim(0, 3.5)
        axes[index].set_ylim(0, 3.5)
        axes[index].tick_params(labelsize=10)
        axes[index].set_xticks(range(0, 4, 1), minor=False)
        axes[index].set_yticks(range(0, 4, 1), minor=False)
        axes[index].set_xlabel('Shannon Entropy,\nUnmodified Packer', fontsize=12)
        axes[index].set_ylabel(f'Shannon Entropy,\nspecial_rot = {weight}', fontsize=12)

        if axes[index].get_legend():
            axes[index].get_legend().remove()

    fig.savefig(os.path.join(entropy_scatterplot_dir, f'EntropyScatter_MainText.png'), bbox_inches='tight', dpi=300)
    plt.close()


def polar_charged_incorporation(args):
    """
    Demonstrate improved incorporation of polar/charged residues against native packer
    Scatterplot % packer vs. % comprotset
    """
    pass


def design_metric_histograms(args):
    """
    Generate figures for design metrics

    Usage:
      bsff benchmark design_metrics <benchmark_dir> [options]

    Arguments:
      <benchmark_dir>           Root Directory

    Options:
      -b, --hbonds              Look for ligand hbonds counts
      -c, --complex_column      Create complex column derived from output PDB name
      -h, --holes               Use Holes filter
      -w, --weights             Only plot the specified weights
    """
    benchmark_dir = args['<benchmark_dir>']

    # Consolidate design_metrics.csv paths
    csv_path_list = list()
    for root, dirs, files in os.walk(benchmark_dir):
        if 'design_metrics.csv' in files:
            csv_path_list.append(os.path.join(root, 'design_metrics.csv'))

    # Combine all design_metrics into one fat dataframe
    df_list = [pd.read_csv(csv) for csv in csv_path_list]
    df = pd.concat(df_list)

    # Create complex column because I totally forgot...
    def return_complex(row):
        _, file = os.path.split(row['path'])
        filename, _ = os.path.splitext(file)
        filename_split = filename.split('-')
        complex = f'{filename_split[0]}-{filename_split[1]}'
        return complex

    def return_specialrot(row):
        if row['comprotset'] is True:
            return row['special_rot_weight']
        else:
            return 'Control'

    if 'match' not in df.keys():
        df['match'] = df.apply(lambda row: return_complex(row), axis=1)

    df['specialrot'] = df.apply(lambda row: return_specialrot(row), axis=1)

    metrics = {0: {0: 'bindingstrain',
                   1: 'residueie',
                   2: 'packstat',
                   3: 'heavyburiedunsats',
                   },
               1: {0: 'ligand_sasa',
                   1: 'shapecomplementarity'}
               }

    if args['--holes']:
        metrics[1][2] = 'holes'
    if args['--hbonds']:
        metrics[1][3] = 'hbonds'

    for complex, complex_df in df.groupby(['match']):
        fig, axes = plt.subplots(2, 4, figsize=(8, 4), constrained_layout=True)
        for x in metrics:
            for y in metrics[x]:

                if (x, y) != (1, 3):
                    sns.violinplot(complex_df[metrics[x][y]], complex_df['specialrot'],
                                   order=['Control', 0.0, -1.5, -3.0, -4.0], ax=axes[x, y])
                else:
                    for weight, weight_df in complex_df.groupby(['specialrot']):
                        plot_kde = True if len(weight_df['hbonds'].unique()) > 1 else False
                        sns.distplot(weight_df['hbonds'], ax=axes[x, y], kde=plot_kde, kde_kws={"label": f'{weight}'})

                if y != 0:
                    axes[x, y].set_ylabel('')

                if (x, y) == (0, 0):
                    handles, labels = axes[x, y].get_legend_handles_labels()
                if axes[x, y].get_legend():
                    axes[x, y].get_legend().remove()

        lgd = fig.legend(handles, labels, loc='center right', title='special_rot', ncol=1, bbox_to_anchor=(1.15, 0.5))
        ttl = fig.suptitle(f'{complex}')
        fig.savefig(f'{complex}-designmetrics.png', bbox_extra_artist=[lgd, ttl], bbox_inches='tight', dpi=300)

    # Print Agg median/counts
    # Maintext figures
    maintext_metrics = {0: {0: 'shapecomplementarity',
                            1: 'holes',
                            2: 'ligand_sasa',
                            }
                        }

    xlabels = {0: 'Shape\nComplementarity',
               1: 'RosettaHoles',
               2: 'Ligand SASA (\u212b\u00b2)',
               }

    # Duplicate code because I'm so over this
    for complex, complex_df in df.groupby(['match']):
        fig, axes = plt.subplots(1, 4, figsize=(7, 2.5), constrained_layout=True, sharey='row')
        for x in maintext_metrics:
            for y in maintext_metrics[x]:

                plot_order = [-4.0, -3.0, -1.5, 0.0, 'Control']
                sns.violinplot(complex_df[maintext_metrics[x][y]], complex_df['specialrot'], order=plot_order, ax=axes[y])
                axes[y].set_xlabel(xlabels[y])
                if y == 0:
                    axes[y].set_ylabel('special_rot (REU)')
                else:
                    axes[y].set_ylabel('')

                if axes[y].get_legend():
                    axes[y].get_legend().remove()
        axes[3].invert_yaxis()

        ttl = fig.suptitle(f'{complex}')
        fig.savefig(f'Maintext-{complex}-designmetrics.png', bbox_extra_artist=[ttl], bbox_inches='tight', dpi=300)

    # Joint Plots for maintext metrics


def alignment_sequence_logo(args):
    """
    Generate sequence logo for designed positions

    Usage:
      bsff benchmark seqlogos <seqlogo_dir> [options]

    Arguments:
      <seqlogo_dir>     Directory containing FASTAs for seqeunce logos and designable position pickle
    """

    seqlogo_dir = args['<seqlogo_dir>']
    design_positions_dict = pickle.load(open(os.path.join(seqlogo_dir, 'DesignPositions.pickle'), 'rb'))

    seqlogo_out_dir = 'SequenceLogos'
    os.makedirs(seqlogo_out_dir, exist_ok=True)

    def make_seqlogo(fasta_path, outdir='SequenceLogos'):
        os.makedirs(outdir, exist_ok=True)
        root, fastafile = os.path.split(fasta_path)
        fasta_name, _ = os.path.splitext(fastafile)

        seqs = weblogo.seq_io.read(open(fasta_path, 'r'))
        seqs.alphabet = unambiguous_protein_alphabet
        logodata = weblogo.logo.LogoData.from_seqs(seqs)
        logooptions = weblogo.logo.LogoOptions()
        logooptions.title = f"{fasta_name}"

        logooptions.logo_margin = 2
        logooptions.xaxis_tic_interval = 1
        logooptions.number_interval = 1
        logooptions.fontsize = 36
        logooptions.number_fontsize = 24
        logooptions.stacks_per_line = 100
        logooptions.stack_width = 50
        logooptions.resolution = 300

        logooptions.show_errorbars = False
        logooptions.show_fineprint = False
        logooptions.scale_width = False
        logooptions.show_boxes = False

        logooptions.annotate = complex_design_positions
        logoformat = weblogo.logo.LogoFormat(logodata, logooptions)

        logo_binary = weblogo.logo_formatter.eps_formatter(logodata, logoformat)
        sequencelogo_out_path = os.path.join(outdir, f'{fasta_name}.eps')
        print(sequencelogo_out_path)
        with open(sequencelogo_out_path, 'wb') as f:
            f.write(logo_binary)

    for complex in os.listdir(seqlogo_dir):
        complex_dir = os.path.join(seqlogo_dir, complex)
        if os.path.isdir(complex_dir):
            complex_design_positions = design_positions_dict[complex]['positions']
            for fasta in os.listdir(complex_dir):
                if fasta.endswith('.fasta'):
                    make_seqlogo(os.path.join(complex_dir, fasta))

            # Make logo for native sequence
            native_seq = SeqRecord(Seq(''.join(design_positions_dict[complex]['sequence']), IUPAC.protein))
            with tempfile.TemporaryDirectory() as temp_fasta:
                temp_fasta = os.path.join(temp_fasta, f'{complex}-Native.fasta')
                SeqIO.write([native_seq], temp_fasta, 'fasta')
                make_seqlogo(temp_fasta)



